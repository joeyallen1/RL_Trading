{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mConnection is disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Environments import TrainingEnv\n",
    "from Environments import TestingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('KO_Data.csv')\n",
    "data.drop(labels=['Date'], axis=1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data['Close'] = data['Close'].replace(to_replace=0.0, method='ffill')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[:int(0.7 * len(data)), :].copy(deep=True)\n",
    "validation_data = data.iloc[int(0.7 * len(data)):int(0.8 * len(data)), :].copy(deep=True)\n",
    "testing_data = data.iloc[int(0.8 * len(data)):, :].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = TrainingEnv(training_data, episode_length=500)\n",
    "check_env(training_env, warn=True)\n",
    "validation_env = TestingEnv(validation_data)\n",
    "check_env(validation_env, warn=True)\n",
    "testing_env = TestingEnv(testing_data)\n",
    "check_env(testing_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(validation_env, eval_freq=100, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# model = DQN(\"MlpPolicy\", training_env, buffer_size=5000, learning_rate=0.005, seed=6)\n",
    "# model.learn(int(1e10), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\\\n",
    "\n",
    "eval_callback = EvalCallback(training_env, eval_freq=500, verbose=1)\n",
    "\n",
    "model = DQN(\"MlpPolicy\", training_env, buffer_size=5000, seed=6, exploration_fraction=0.3, tensorboard_log=\"./tensorboard\")\n",
    "model.learn(500*100, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_performance(env, data):\n",
    "    portfolio_values = []\n",
    "    budget = 10000\n",
    "    budgets = []\n",
    "    obs, info = env.reset()\n",
    "    asset_allocations = []\n",
    "    actions = []\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        percent_change = (data.iloc[env.cur_row_num, 0] - data.iloc[env.cur_row_num-1, 0]) / data.iloc[env.cur_row_num-1, 0]\n",
    "        budget = budget * (1+percent_change)\n",
    "        budgets.append(budget)\n",
    "        portfolio_values.append(info['Portfolio Value'])\n",
    "        asset_allocations.append(info['Asset Allocation'])\n",
    "        actions.append(info['Action Taken'])\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    plt.figure()\n",
    "    plt.plot(np.array(asset_allocations))\n",
    "    plt.figure()\n",
    "    plt.hist(np.array(actions), bins=5)\n",
    "    plt.figure()\n",
    "    plt.plot(np.array(portfolio_values), \"blue\")\n",
    "    plt.plot(np.array(budgets), \"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(validation_env, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps: continue experimenting with reward function scaling and exploration fraction\n",
    "# also think about using optuna for hyperparameter tuning\n",
    "# think more about how to handle divide by zero errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# import optuna\n",
    "\n",
    "# max_return = 0.0\n",
    "\n",
    "# def objective(trial):\n",
    "#     global max_return\n",
    "#     learning_rate = trial.suggest_float('learning rate', 0.0001, 0.01)\n",
    "#     gamma = trial.suggest_float('gamma', 0.95, 0.999)\n",
    "#     exploration_fraction = trial.suggest_float('exploration fraction', 0.1, 0.3)\n",
    "#     total_timesteps = trial.suggest_int('total_timesteps', 250 * 50, 250 * 1000)\n",
    "\n",
    "\n",
    "#     model = DQN(\"MlpPolicy\", training_env, learning_rate=learning_rate, \n",
    "#                 buffer_size=5000, gamma=gamma, exploration_fraction=exploration_fraction)\n",
    "    \n",
    "\n",
    "#     model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "#     obs, info = validation_env.reset()\n",
    "#     while True:\n",
    "#         action, _states = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, terminated, truncated, info = validation_env.step(action)\n",
    "#         if terminated or truncated:\n",
    "#             break\n",
    "\n",
    "#     raw_return = (validation_env.portfolio_value - validation_env.budget) / validation_env.budget\n",
    "#     if raw_return > max_return:\n",
    "#         model.save(\"RL_Agent\")\n",
    "#         max_return = raw_return\n",
    "#     del model\n",
    "#     return -raw_return\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# study = optuna.create_study()\n",
    "# study.optimize(objective, n_trials = 20, show_progress_bar=True)\n",
    "\n",
    "# print(f'Best Parameters: {study.best_params}')\n",
    "# print(f'Best value: {study.best_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
